# Design-and-Development-of-AI-based-Nursing-Robot
To develop a nursing robot that can detect and provide and dispense timely medicines to the patients in need.
## Abstract
Recent events have demonstrated how robotics’ involvement in healthcare and related fields is evolving, with special worries regarding the management and control of the transmission of viruses that can cause pandemics. Such robots are mostly used in hospitals and other similar settings, such as quarantines, to minimise direct human contact and to ensure cleaning, sterilisation, and assistance. As a result, the risk to the lives of medical professionals and doctors participating actively in pandemic management will be reduced. Overall, even though nurse robots have a lot of potential uses and advantages, it’s critical to keep in mind their drawbacks, such the absence of human contact, and make sure they work in tandem with human carers to give patients the best care possible. The system’s goal is to deliver medications based on patient information and established routes. Facial detection can help to identify a particular patient. The robot is designed to perform various tasks related to patient care in healthcare settings such as assisting healthcare professionals in delivering high-quality care to patients, improve patient safety, and increase efficiency in healthcare delivery. Nurse robots can help with tasks such as monitoring vital signs, administering medication, providing physical therapy, and assisting with activities of daily living. The design and creation of a nurse robot utilising a Raspberry Pi are covered in this project. HAAR cascasde and LPBH algorithms are implemented for facial detection and recognition to identify the patient. The robot dispenses medicines based on the identity of the patient. Sentiment analysis using AI, i.e a CNN emotion detection model is implemented in order to estimate the mental state of the patient. These robots enhance efficiency by automating routine tasks, leading to a 30-40% reduction in nurses’ workload. Additionally, nurse robots have the potential to generate cost savings of 15-20% in overall healthcare expenditures. An accuracy of 95.2% is obtained for Haar Cascade and 93.9% for Local Binary Pattern Histogram (LBPH) classifier while training the facial data for recognition and detection. Emotion detection had an accuracy of 63.66%.
## Objectives
* To develop an Automated self-learning Robot for medical treatment and predictive analytics. 
* To equip working of robot through wireless , Internet Of Things (IOT) and Sensor Technologies relevant to Medical Treatment 
* To implement sentiment analysis and online messaging service to improve capability of the robot
## Methodology
The robot project involves multiple stages, including dataset collection through continuous video capture and face detection using the Haar Cascade algorithm. This dataset is used to train a reliable facial recognition model with the LPBH algorithm, enabling accurate patient identification.The project also focuses on the movement of the robot using Raspberry Pi, motors, and sensors, enabling it to interact with patients, perform facial recognition, record vital signs, and facilitate medication collection. Furthermore, the robot communicates important information by sending Telegram messages, providing patients with precise readings of temperature and heart rate.Additionally, the robot incorporates advanced emotion detection capabilities with a CNN model, allowing it to analyze facial expressions and respond empathetically.By combining these elements, the robot aims to enhance patient experiences, ensure accurate identification, and deliver compassionate care in healthcare settings.
The methology of the robot consists of the following steps:<br />
**Dataset Collection**<br />
The first step of the execution process involves assigning a unique ID to each patient. To initiate this process, a Raspberry Pi camera module is utilized to capture a continuous video stream. Each frame of the video is then extracted for further analysis. In order to perform face detection, the Haar Cascade algorithm is implemented. Intel’s Pretrained detector, which is a model developed by Rainer on behalf of Intel, is used for this purpose. The model is created after undergoing intensive training to accurately detect faces in images. The model is represented by an XML file that contains the necessary parameters and patterns for face detection. To facilitate face detection, the images are first converted into grayscale. This conversion simplifies the processing and analysis of the images, as grayscale images only consist of shades of gray rather than color variations. Once the images are converted, the detectMultiScale function is used to detect faces within the image. The detectMultiScale function takes several parameters to customize the face detection process. These parameters include the scaling factor and minNeighbors. The scaling factor determines the size of the image regions to be analyzed during the detection process. A scaling factor of 1.3, means that the algorithm will progressively downscale the image by 30percent to search for faces at different scales. This allows for the detection of faces of various sizes within the image. The minNeighbors parameter sets the minimum number of neighboring bounding boxes needed to classify a region as a face. It helps filter out false-positive detections and improves the accuracy of the face detection process. Higher values for minNeighbors tend to produce fewer detections but with higher confidence, reducing the chances of incorrect face identifications. During this step, 100 images are recorded per patient along with their assigned ID. This comprehensive dataset ensures a robust collection of images for further analysis, such as facial recognition or other related tasks. Each image is associated with the respective patient’s ID, enabling individual identification throughout the execution process. In summary, the first step of the execution process involves assigning unique IDs to patients and capturing continuous video using a Raspberry Pi camera module. The Haar Cascade algorithm, implemented with Intel’s Pretrained detector, is employed for face detection. The images are converted to grayscale, and the detectMultiScale function is utilized to detect faces using the specified parameters. Finally, 100 images per patient are recorded along with their assigned IDs, creating a comprehensive dataset for subsequent analysis and tasks.<br />
**Model Training**<br />
In the second step of the execution process, the dataset created in the previous step is utilized. This step focuses on face recognition using the Haar cascade frontal face detector for face detection and the LBPH algorithm for face recognition. To begin, the OpenCV API, specifically the LBPHFaceRecognizer-create function, is employed to create an LPBH recognizer model. This model will be trained to recognize faces based on the provided dataset. Each image in the dataset is first converted into grayscale. This conversion simplifies the processing and analysis of the images, as grayscale images only contain shades of gray rather than color information. The grayscale images are then transformed into arrays, which can be processed by the face recognition algorithm. The Haar detector is then used to detect faces within each grayscale image. The detectMultiScale function, specifically the detect.Multiscale function in this case, is applied to search for faces at different scales within the image. When a face is detected, it is enclosed in a bounding box, and the face region along with its respective ID is recorded in separate arrays for further processing. For each image in the dataset, this process is repeated to capture all the faces present in the image and record their corresponding IDs. This step ensures that the dataset contains the necessary information for training the face recognition model. Next, the recognizer model is trained using the collected face images and their respective IDs. The training process involves analyzing the patterns and features present in the face images to create a representation or histogram that characterizes each individual face. The recognizer model learns to associate these histograms with their corresponding IDs, enabling it to identify individuals based on new face images. Finally, the trained model is saved as a yml file. This file contains the histograms and associated IDs, serving as a reference for future face recognition tasks. By saving the trained model, it can be easily loaded and utilized in subsequent executions without the need for retraining. In summary, the second step of the execution process utilizes the dataset created in the previous step. It involves creating an LPBH recognizer model using the OpenCV API, converting the images to grayscale, detecting faces using the Haar cascade algorithm, and recording the faces and their respective IDs. The recognizer model is then trained using the collected data, and the trained model is saved as a yml file for future use.<br />
**Facial Recognition**<br />
In the main step, the robot, built with Raspberry Pi, motors, motor drivers, and sensors, begins its movement by comparing the system time with the specified time in the code. Using the L293D motor driver IC, the robot controls its wheels. It follows predefined paths to reach the patient and instructs them to look into the camera. The robot detects faces using the HAAR cascade algorithm and recognizes the patient with the previously trained LPBH model. If recognized, the patient is prompted to place their finger on temperature and heart rate sensors connected to the Raspberry Pi via MCP3008 Analog-to-Digital Converter (ADC). After recording the sensor values, the robot informs the patient and sends a telegram message with their name, temperature, and heart rate values. It opens the door, informs the patient to collect their medicines, and proceeds to the next patient. If the patient is not recognized, an appropriate message is played, and the robot moves on to the next patient.<br />
**Emotion Dection**<br />
In this scenario, a pre-trained model is being used to detect seven different emotions: Angry, Disgusted, Fearful, Happy, Neutral, Sad, and Surprised. The model is stored as a JSON file, which is read from its directory and loaded using the function model-from-json. Figure 4.3 depicts the execution of the emotion detection pipeline. The model used in the project is a CNN architecture consisting of Convolutional, Maxpooling, Dropout, Flatten, and Dense layers. The summary function provides an overview of the model’s structure, output shapes, and the number of parameters involved. The model’s layers perform spatial feature extraction, reduce dimensions, prevent overfitting, and classify data. Once the model structure is loaded, the pre-trained weights are loaded into the model using the function load-weights. This step ensures that the model is equipped with the learned patterns and features necessary for emotion detection. To capture emotions in real-time, a continuous video stream is captured, and frames are extracted from it. These frames are then preprocessed by converting them into grayscale, reducing the complexity of the image and focusing on the intensity variations. Grayscale conversion helps in simplifying the subsequent face detection process. The Haar cascade frontal face detector is utilized to detect faces within each frame. This face detection algorithm, implemented through the detectMultiScale function, scans the frame with a specified scaling factor and minimum neighbors, searching for facial features that match a predefined pattern. When a face is detected, its coordinates are recorded. For every frame, a region of interest is defined within the grayscale version of the frame, corresponding to the previously detected face coordinates. This ROI represents the specific area containing the face, where emotions will be analyzed. Next, the ROI grayscale image is resized to a fixed size of (48, 48) using the cv2.resize function. Resizing ensures that the input dimensions match the expected size of the emotion model. The model requires a consistent input shape to make accurate predictions. The preprocessed face image is then fed into the emotion model for prediction using the predict method. This method generates predicted probabilities for each emotion class based on the provided input. The output is a probability distribution across the seven emotions. To determine the emotion with the highest predicted probability, the np.argmax function is employed. It returns the index of the emotion class with the highest probability, indicating the most likely detected emotion. Finally, the detected emotion is added as text to the frame displaying the faces. This visual representation allows viewers to see the detected emotion alongside the corresponding face, providing an intuitive understanding of the model’s predictions.

## Execution
* Loading model and weights: The pre-trained CNN model and its weights are loaded into memory. It can detect the following emotions: Angry, Disgusted, Fearful, Happy, Neutral, Sad, and Surprised.
* Face Detection: The HAAR cascade algorithm is utilized to detect faces within the image.
* Preprocessing: Converting the images into greyscale format in order to reduce complexity.
* Feed to model and predict emotions: The face images are passed to the model, which generates a probability value for each emotion class. The highest probability indicates the predicted emotion for each detected face.
